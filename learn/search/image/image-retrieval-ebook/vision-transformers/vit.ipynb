{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panosdrama/randomise_programme_from-python_-to-excel/blob/random_programme/learn/search/image/image-retrieval-ebook/vision-transformers/vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScJPUvuBZLYl"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/search/image/image-retrieval-ebook/vision-transformers/vit.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/search/image/image-retrieval-ebook/vision-transformers/vit.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ABtSCJ7ZLYn"
      },
      "source": [
        "# Vision Transformers (ViT) Walkthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bOlutUi7ZLYn",
        "outputId": "fcbbc793-04a7-4dfe-9777-430d35c76b12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVP-oaliZLYo"
      },
      "source": [
        "Let's start by downloading the CIFAR-10 dataset from HuggingFace. We will first download the training dataset by setting ```split = 'train'```, and the testing dataset after by setting ```split = 'test'```."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"./cifar\", exist_ok=True)\n",
        "os.makedirs(\"./cifar_logs\", exist_ok=True)\n",
        "cifar_path = os.path.abspath('./cifar')\n",
        "os.makedirs(cifar_path, exist_ok=True)\n",
        "print(os.path.isdir(\"./cifar\"))"
      ],
      "metadata": {
        "id": "h1t1WkcMbLZd",
        "outputId": "dd7b00da-4bbc-4a94-ed4b-fa83a03b3100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "kVwvI_4mZLYo",
        "outputId": "537a9492-7758-40cf-d64c-ad38111d4422",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['img', 'label'],\n",
              "    num_rows: 50000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# import CIFAR-10 dataset from HuggingFace\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_train = load_dataset(\n",
        "    'cifar10',\n",
        "    split='train', # training dataset\n",
        "    #ignore_verifications=False  # set to True if seeing splits Error\n",
        ")\n",
        "\n",
        "dataset_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "zJjP1MMIZLYp",
        "outputId": "cf9ea444-5827-4f9c-a817-fbb3c7ea70ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['img', 'label'],\n",
              "    num_rows: 10000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "dataset_test = load_dataset(\n",
        "    'cifar10',\n",
        "    split='test', # training dataset\n",
        "    #ignore_verifications=True  # set to True if seeing splits Error\n",
        ")\n",
        "\n",
        "dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "27FtLOI0ZLYp",
        "outputId": "cb03931d-7ca2-4d2a-96a2-df364a5efdea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,\n",
              " ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], id=None))"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "# check how many labels/number of classes\n",
        "num_classes = len(set(dataset_train['label']))\n",
        "labels = dataset_train.features['label']\n",
        "num_classes, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwzeasryZLYp"
      },
      "source": [
        "*Training*: 50,000 images divided into 10 classes\n",
        "\n",
        "*Test*: 10,000 images divided into 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "4x0R1xg5ZLYp",
        "outputId": "78af818a-d162-4021-97df-17ac958ad8b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>,\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "dataset_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTudJsn5ZLYq"
      },
      "source": [
        "Those are PIL images with $3$ color channels, and $32x32$ pixels resolution. Let's have a look at the first picture in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "7l_yNFfbZLYq",
        "outputId": "554d5c58-8735-45f5-e5bd-56de325c2b9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
            ],
            "image/png": "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\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDeXxDYm8MHmfKB/rD93PpUsGt2UyufPRdpbAJwSB3rk544YA5cHkhAzLx/9aoAGXAQrn+EOM5FXcDsI9cs3BZpUVd20HPt1NQrrlrcX32SLc+QT5gHy1y5ljcZllUndt35rPeWaF2iDM6Y52jGc0XAt67ezWz+W6J5EjbhID3HbHrQlrqLW8c8MRkikHytCpf88dKimEVyNsyRsByDvIOfWizkuNKlkk0+/miaT7yvIWVj9DSvGw7GrD4c1CWMyfOpbBIaIjmpJPDN5wZZiqY67OlUpdb1W5t2iu79GyB9wbefzqODWb60iWJb1mj7rntU3QWP/9k=\n"
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "dataset_train[0]['img']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "67d3NYKXZLYr",
        "outputId": "279cd5c2-da36-40cf-c4f6-7040d32f3bea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 'airplane')"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "dataset_train[0]['label'], labels.names[dataset_train[0]['label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i01yuMzZLYr"
      },
      "source": [
        "### Loading ViT Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNBJEzjRZLYs"
      },
      "source": [
        "We use `google/vit-base-patch16-224-in21k` model from the Hugging Face Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WruLl4oRZLYs"
      },
      "source": [
        "The model is named as so as it refers to base-sized architecture with patch resolution of 16x16 and fine-tuning resolution of 224x224.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ZqSErHubZLYs"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "\n",
        "# import model\n",
        "model_id = 'google/vit-base-patch16-224-in21k'\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
        "    model_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKCJTJxhZLYs"
      },
      "source": [
        "You can see the feature extractor configuration by printing it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "qRZy3gJeZLYt",
        "outputId": "421c8c42-0268-4775-c327-59d99ee19e90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTFeatureExtractor {\n",
              "  \"do_normalize\": true,\n",
              "  \"do_rescale\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"image_mean\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"image_processor_type\": \"ViTFeatureExtractor\",\n",
              "  \"image_std\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"resample\": 2,\n",
              "  \"rescale_factor\": 0.00392156862745098,\n",
              "  \"size\": {\n",
              "    \"height\": 224,\n",
              "    \"width\": 224\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "feature_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fzv09QAZLYt"
      },
      "source": [
        "If we consider the first image, i.e., the airplane shown above, we can see the resulting tensor after passing the image through the feature extractor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "nrprtbwhZLYt",
        "outputId": "7e165829-fcf1-44d4-b254-39ca0a89c4f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pixel_values': tensor([[[[ 0.3961,  0.3961,  0.3961,  ...,  0.2941,  0.2941,  0.2941],\n",
              "          [ 0.3961,  0.3961,  0.3961,  ...,  0.2941,  0.2941,  0.2941],\n",
              "          [ 0.3961,  0.3961,  0.3961,  ...,  0.2941,  0.2941,  0.2941],\n",
              "          ...,\n",
              "          [-0.1922, -0.1922, -0.1922,  ..., -0.2863, -0.2863, -0.2863],\n",
              "          [-0.1922, -0.1922, -0.1922,  ..., -0.2863, -0.2863, -0.2863],\n",
              "          [-0.1922, -0.1922, -0.1922,  ..., -0.2863, -0.2863, -0.2863]],\n",
              "\n",
              "         [[ 0.3804,  0.3804,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
              "          [ 0.3804,  0.3804,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
              "          [ 0.3804,  0.3804,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
              "          ...,\n",
              "          [-0.2471, -0.2471, -0.2471,  ..., -0.3412, -0.3412, -0.3412],\n",
              "          [-0.2471, -0.2471, -0.2471,  ..., -0.3412, -0.3412, -0.3412],\n",
              "          [-0.2471, -0.2471, -0.2471,  ..., -0.3412, -0.3412, -0.3412]],\n",
              "\n",
              "         [[ 0.4824,  0.4824,  0.4824,  ...,  0.3647,  0.3647,  0.3647],\n",
              "          [ 0.4824,  0.4824,  0.4824,  ...,  0.3647,  0.3647,  0.3647],\n",
              "          [ 0.4824,  0.4824,  0.4824,  ...,  0.3647,  0.3647,  0.3647],\n",
              "          ...,\n",
              "          [-0.2784, -0.2784, -0.2784,  ..., -0.3961, -0.3961, -0.3961],\n",
              "          [-0.2784, -0.2784, -0.2784,  ..., -0.3961, -0.3961, -0.3961],\n",
              "          [-0.2784, -0.2784, -0.2784,  ..., -0.3961, -0.3961, -0.3961]]]])}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "example = feature_extractor(\n",
        "    dataset_train[0]['img'],\n",
        "    return_tensors='pt'\n",
        ")\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "6U0z-_cxZLYt",
        "outputId": "dbe013f0-26d1-4b00-f5d6-ac51527eb2cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "example['pixel_values'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "PxGmgo9NZLYu",
        "outputId": "ea9e8fc8-b4f2-46e3-a00a-af21ea82c03a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "# load in relevant libraries, and alias where appropriate\n",
        "import torch\n",
        "\n",
        "# device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "J-7jFO5mZLYu"
      },
      "outputs": [],
      "source": [
        "def preprocess(batch):\n",
        "    # take a list of PIL images and turn them to pixel values\n",
        "    inputs = feature_extractor(\n",
        "        batch['img'],\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    # include the labels\n",
        "    inputs['label'] = batch['label']\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEmSehBiZLYu"
      },
      "source": [
        "We can apply this to both the training and testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "FPLzX5ErZLYu"
      },
      "outputs": [],
      "source": [
        "# transform the training dataset\n",
        "prepared_train = dataset_train.with_transform(preprocess)\n",
        "# ... and the testing dataset\n",
        "prepared_test = dataset_test.with_transform(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmHXEbDNZLYu"
      },
      "source": [
        "Now, whenever you get an example from the dataset, the transform will be applied in real time (on both samples and slices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hMgso3lZLYu"
      },
      "source": [
        "### Model Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvrpWlkvZLYv"
      },
      "source": [
        "In this section, we are going to build the Trainer, which is a feature-complete training and eval loop for PyTorch, optimized for HuggingFace 🤗 Transformers.\n",
        "\n",
        "We need to define all of the arguments that it will include:\n",
        "* training and testing dataset\n",
        "* feature extractor\n",
        "* model\n",
        "* collate function\n",
        "* evaluation metric\n",
        "* ... other training arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMvGYayNZLYv"
      },
      "source": [
        "The collate function is useful when dealing with lots of data. Batches are lists of dictionaries, so collate will help us create batch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "jViU73tsZLYv"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['label'] for x in batch])\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbVIaZWcZLYv"
      },
      "source": [
        "Let's now define the evaluation metric we are going to use to compare prediction with actual labels. We will use the *accuracy evaluation metric*.\n",
        "\n",
        "Accuracy is defined as the proportion of correct predictions (True Positive ($TP$) and True Negative ($TN$)) among the total number of cases processed ($TP$, $TN$, False Positive ($FP$), and False Negative ($FN$)).\n",
        "\n",
        "$$Accuracy = \\frac{(TP + TN)}{(TP + TN + FP + FN)}$$    \n",
        "\n",
        "Below, we are using accuracy within the ```compute_metrics``` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "3dE75GH-ZLYv",
        "outputId": "1280cc19-e146-4d00-f95b-6ae9106a6aba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'load_metric' from 'datasets' (/usr/local/lib/python3.10/dist-packages/datasets/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-b7bbca42f0bd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# accuracy metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_metric' from 'datasets' (/usr/local/lib/python3.10/dist-packages/datasets/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "# accuracy metric\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(\n",
        "        predictions=np.argmax(p.predictions, axis=1),\n",
        "        references=p.label_ids\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NKriWgwZLYw"
      },
      "source": [
        "The last thing consists of defining ```TrainingArguments```.\n",
        "\n",
        "Most of these are pretty self-explanatory, but one that is quite important here is ```remove_unused_columns=False```. This one will drop any features not used by the model's call function. By default it's True because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('image' in particular) in order to create 'pixel_values'.\n",
        "\n",
        "We have chosen a batch size equal to 16, 100 evaluation steps, and a learning rate of $2e^{-4}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AliOX35ZZLYw"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./cifar\",\n",
        "  per_device_train_batch_size=16,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=4,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ojKya-GZLYw"
      },
      "source": [
        "We can now load the pre-trained model. We'll add ```num_labels``` on init so the model creates a classification head with the right number of units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Wnx4_rZLYw"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n",
        "labels = dataset_train.features['label'].names\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_id,  # classification head\n",
        "    num_labels=len(labels)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biB41-N4ZLYw"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vu65jxXZLYx"
      },
      "source": [
        "We can see the characteristics of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crTs0S22ZLYx"
      },
      "source": [
        "Now, all instances can be passed to ```Trainer```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyZQweufZLYx"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=prepared_train,\n",
        "    eval_dataset=prepared_test,\n",
        "    tokenizer=feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7qpeTyrZLYx"
      },
      "source": [
        "We can save our trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omOCWYulZLYx"
      },
      "outputs": [],
      "source": [
        "train_results = trainer.train()\n",
        "# save tokenizer with the model\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "# save the trainer state\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdpPZ2sKZLYx"
      },
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzBcDBUfZLYx"
      },
      "source": [
        "We can now evaluate our model using the accuracy metric defined above..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJberw4HZLYy"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate(prepared_test)\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8L5HwFLZLYy"
      },
      "source": [
        "Model accuracy is pretty good. Let's have a look to an example. We can pick the first image in our testing dataset and see if the predicted label is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AR8Dv6uZLYy"
      },
      "outputs": [],
      "source": [
        "# show the first image of the testing dataset\n",
        "image = dataset_test[\"img\"][0].resize((200,200))\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeHGgFxbZLYy"
      },
      "source": [
        "The image is not very clear, even when resized. Let's extract the actual label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cudc4Q6BZLYy"
      },
      "outputs": [],
      "source": [
        "# extract the actual label of the first image of the testing dataset\n",
        "actual_label = dataset_test[\"label\"][0]\n",
        "\n",
        "labels = dataset_test.features['label']\n",
        "actual_label, labels.names[actual_label]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVvg82sNZLYz"
      },
      "source": [
        "It looks like the image represents a cat. Let's now see what our model has predicted. Given we saved it on the HuggingFace Hub, we first need to import it. We can use ViTForImageClassification and ViTFeatureExtractor to import the model and extract its features. We would need the predicted pixel values \"pt\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4eMYFZFZLYz"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "\n",
        "# import our fine-tuned model\n",
        "model_name_or_path = 'LaCarnevali/vit-cifar10'\n",
        "model_finetuned = ViTForImageClassification.from_pretrained(model_name_or_path)\n",
        "# import features\n",
        "feature_extractor_finetuned = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNGXdrR1ZLYz"
      },
      "outputs": [],
      "source": [
        "inputs = feature_extractor_finetuned(image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model_finetuned(**inputs).logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9oGgLBZZLYz"
      },
      "source": [
        "We can now see what is our predicted label. Do extract it, we can use the argmax function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Ibf4u7ZLYz"
      },
      "outputs": [],
      "source": [
        "predicted_label = logits.argmax(-1).item()\n",
        "labels = dataset_test.features['label']\n",
        "labels.names[predicted_label]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfLLbB7iZLY0"
      },
      "source": [
        "And the answer is cat. Which is what we would expect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUTfyyQSZLY0"
      },
      "source": [
        "## References\n",
        "\n",
        "[Article](https://pinecone.io/learn/vision-transformers/)\n",
        "\n",
        "[1] Dosovitskiy et al., [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929), 2021, CV.\n",
        "\n",
        "[2] Vaswani et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017.\n",
        "\n",
        "[3] Saeed M., [A Gentle Introduction to Positional Encoding in Transformer Models, Part 1](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/), 2022, Attention, Machine Learning Mastery."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5fe10bf018ef3e697f9035d60bf60847932a12bface18908407fd371fe880db9"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}